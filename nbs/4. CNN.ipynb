{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "* What's overfitting?\n",
    "    * Regularization\n",
    "* Normalization\n",
    "    * Use the same STD and Mean for Test set\n",
    "* MSE and MAE?\n",
    "* Softmax?\n",
    "* Bias vs. Variance trade-off\n",
    "* Exponentially Moving Weighted Average [(read more)](https://medium.com/datadriveninvestor/exponentially-weighted-average-for-deep-neural-networks-39873b8230e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "**assuming** human-error (optimal / bayes error) ~ 0%\n",
    "\n",
    "\n",
    "        Error\n",
    "        \n",
    "    Train |       1 %      |      15 %        |       15 %       |       0.5 %\n",
    "\n",
    "------\n",
    "    Test  |       11 %     |      16 %        |       30 %       |        1 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "-----\n",
    "                      High Variance  |   High Bias      |   High Bias       |      Low Bias\n",
    "                      (Overfitting)  | (Underfitting)   |   High Variance   |      Low Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What if optimal error is 15%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializating Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What parameters need initializations?\n",
    "* Can we initialize our weights in NeuralNetworks as `Zero`? What will happen? (Symmetry breaking problem)\n",
    "    * Both the forward and backward pass of two neurons will be identical (?)\n",
    "    * If both are identical, hence they are computing the same function!\n",
    "* Why can't we initialize the weight matrices as we like?\n",
    "    * Vanishing / Exploding gradient\n",
    "* Type of random initializations\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Weight Initialization\n",
    "w = np.random.randn((2,2)) * 0.01 # Why 0.01? Saturation -> Small Gradient Descent -> Slow Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glorot / Xavier Uniform and Normal \n",
    "F_in = 64 # Fan in\n",
    "F_out = 32 # Fan out\n",
    "limit = np.sqrt(2 / float(F_in + F_out))\n",
    "W = np.random.normal(0., limit, size = (F_in, F_out))\n",
    "\n",
    "limit_uni = np.sqrt(6 / float(F_in + F_out))\n",
    "W_uni = np.random.uniform(low = -limit, high = limit, size = (F_in, F_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# He et al. / Kaiming Uniform and Normal\n",
    "# Deep Networks where ReLU like activation functions are in-use\n",
    "\n",
    "F_in = 64\n",
    "F_out = 32\n",
    "limit = np.sqrt(6 / float(F_in))\n",
    "W = np.random.uniform(low = -limit, high = limit, size = (F_in, F_out))\n",
    "\n",
    "limit_norm = np.sqrt(2 / float(F_in))\n",
    "W_norm = np.random.randn(0., limit, size = (F_in, F_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    “Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are collectively known as regularization.” – Goodfellow et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While loss function determines how well/poorly we are doing on the given task, and how *gradient descent* updates our weight parameters, both has nothing to do with the `look` of weight matrix. Taking into account that there are almost *infinite* amount of parameters that will achieve virtually the same accuracy.\n",
    "\n",
    "\n",
    "So, How we choose set of *parameters* so that our model **generalize** well? In other words, reduce the amount of overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Regularization?\n",
    "* Generalization: Making sure that our models can better handle the unseen data.\n",
    "* Risks of generalization: Too much generalization leads to *underfitting*, in which model could not pick the relationship between input and output, and has a poor performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it works?\n",
    "\n",
    "* Simplifies the network (kind of wrong analogy)\n",
    "* Reduces the effect of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Regularizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L1 Regularization (Ridge)\n",
    "* L2 Regularization (Lasso / Weight Decay)\n",
    "* Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reguralizations that can be **explicitly** added to the network such as **dropout**\n",
    "\n",
    "* Dropout\n",
    "    * Trains small parts of the network\n",
    "    * Higher drop-rate for complex layers (layers with too many neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, **implicit** ones that are applied *during* the training process. E.g. **Augmentation**, and **Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Traditional NN: We used Fully-Connected layers\n",
    "    * FC: Each neuron in layer *`i`* is connected to every neuron in layer *`i - 1`*\n",
    "* Convolution NN: Atleast, one of those FC layers is replaced with *Convolutional* layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why convolutions?\n",
    "\n",
    "1. Sparse Interactions\n",
    "    1. Using smaller # of inputs to extract meaningful structures from data (Image m x n --> Kernel k x n)\n",
    "    \n",
    "    <img src=\"./img/sparse_connection_cnn.png\" width = \"400\" height = \"350\">\n",
    "    [source](IanGoodFellow)\n",
    "\n",
    "2. Parameter Sharing\n",
    "    1. Weights, in kernels, are shared among all inputs  (**bold** arrows demonstrate shared params)\n",
    "    \n",
    "    <img src= \"./img/param_sharing_cnn.png\" width = '400' height = '350'>\n",
    "\n",
    "3. Equivariant Representations\n",
    "    1. Layer is unaffected by images subject to translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's convolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tiny matrices (compared to the Original image) that are slided over the big matrix\n",
    "\n",
    "* Why the size is odd? (3x3, 5x5)\n",
    "    * Locate origin in each matrix.\n",
    "\n",
    "<img src='./img/kernels_odd_cnn.png' width = '400' height = '300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Element wise matrix multiplication and sum of its elements!\n",
    "<img src='./img/3D_Convolution_Animation.gif' alt='https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=27&cad=rja&uact=8&ved=2ahUKEwjon9OqtPTkAhVDaFAKHZvJCocQFjAaegQIAxAB&url=https%3A%2F%2Fcommons.wikimedia.org%2Fwiki%2FFile%3A3D_Convolution_Animation.gif&usg=AOvVaw2CiZ1QzGPnsWoB-_pOmzH6'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Image Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(visualization)](https://github.com/vdumoulin/conv_arithmetic)\n",
    "   \n",
    "    Image Size : N x N\n",
    "\n",
    "    Kernel / Filter Size : F x F\n",
    "\n",
    "    Output = (N - F + 1) x (N - F + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's Padding?\n",
    "    * Add zeros to borders\n",
    "* Why padding?\n",
    "    * Image shrinkage\n",
    "    * Less data on corners\n",
    "* Valid and Same Convolutions\n",
    "    * Valid: **No** Padding\n",
    "    * Same: **Same** size as input\n",
    "* What's the output size if we have padding of size `p`    \n",
    "\n",
    "        (N + 2P - F + 1) x (N + 2P - F + 1)\n",
    "    * why 2p?\n",
    "* Padding formula (for same output dim)\n",
    "\n",
    "        P = ( F - 1 ) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What happens to the output size if we have stride? (s = stride)\n",
    "\n",
    "$$ {\\lfloor}{\\frac{n + 2p - f}{s} + 1}{\\rfloor} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convolution over volume\n",
    "\n",
    "        Image N x N x C (channels)\n",
    "        Filter F x F x C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have 10 filters of kernels with size (3 x 3 x 3) in one layer of neural network, how many parameters we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3 x 3 x 3 = 27\n",
    "\n",
    "    27 x 10 = 270\n",
    "\n",
    "    270 + 10 = 280 parameters\n",
    "                (each filter has a bais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution in DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of what we have used are hand-crafted kernels which was used specifically for each image processing task, but now we can use machine learning to learn values of the kernel *automatically!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convolutional (CONV)\n",
    "* Activation (ACT, or RELU)\n",
    "* Pooling (POOL)\n",
    "* Fully-Connected (FC)\n",
    "* BatchNorm (BN)\n",
    "* Dropout (DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Deep Learning, Ian Good Fellow\n",
    "2. Deep Learning for Computer Vision, Adrian\n",
    "3. Coursera Deep Learning, Andrew Ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
